{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38299ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b162a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train an XGBoost regressor for cell counting.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metadata\",\n",
    "        type=Path,\n",
    "        default=Path(\"processed/metadata.csv\"),\n",
    "        help=\"CSV generated by data_preprocess.py.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--root\",\n",
    "        type=Path,\n",
    "        default=Path(\"processed\"),\n",
    "        help=\"Root directory containing sample NPZ files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--experiment-root\",\n",
    "        type=Path,\n",
    "        default=Path(\"experiments\"),\n",
    "        help=\"Directory where run artifacts will be stored.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--run-name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Optional identifier for this training run (defaults to timestamp).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val-split\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Fraction reserved for validation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-split\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Fraction reserved for final testing.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Number of boosting rounds (mirrors cnn.py epochs).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning-rate\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"XGBoost learning rate (eta).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-depth\",\n",
    "        type=int,\n",
    "        default=6,\n",
    "        help=\"Maximum tree depth.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--subsample\",\n",
    "        type=float,\n",
    "        default=0.8,\n",
    "        help=\"Row subsample ratio per tree.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--colsample-bytree\",\n",
    "        type=float,\n",
    "        default=0.8,\n",
    "        help=\"Column subsample ratio per tree.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reg-alpha\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"L1 regularization term.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reg-lambda\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"L2 regularization term.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min-child-weight\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Minimum sum of instance weight needed in a child.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tree-method\",\n",
    "        type=str,\n",
    "        default=\"hist\",\n",
    "        help=\"Tree construction algorithm.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help=\"Random seed.\",\n",
    "    )\n",
    "    return parser.parse_args(args=[] if argv is None else list(argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46168c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(metadata_path: Path, root: Path) -> List[Path]:\n",
    "    npz_paths: List[Path] = []\n",
    "    with metadata_path.open(\"r\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        for row in reader:\n",
    "            sample_rel = Path(row[\"sample_npz\"])\n",
    "            if sample_rel.is_absolute():\n",
    "                sample_path = sample_rel\n",
    "            elif sample_rel.parts and sample_rel.parts[0] == root.name:\n",
    "                sample_path = (root.parent / sample_rel).resolve()\n",
    "            else:\n",
    "                sample_path = (root / sample_rel).resolve()\n",
    "            npz_paths.append(sample_path)\n",
    "    if not npz_paths:\n",
    "        raise SystemExit(f\"No samples found via {metadata_path}.\")\n",
    "    return npz_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72850c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(npz_paths: Sequence[Path]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    features: List[np.ndarray] = []\n",
    "    targets: List[float] = []\n",
    "    for path in npz_paths:\n",
    "        with np.load(path) as data:\n",
    "            image = data[\"image\"]\n",
    "            features.append(image.reshape(-1))\n",
    "            targets.append(float(np.array(data[\"count\"]).item()))\n",
    "    # store as float16 to cut memory footprint before training\n",
    "    X = np.stack(features).astype(np.float16)\n",
    "    y = np.asarray(targets, dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_run_dir(root: Path, run_name: str | None) -> Path:\n",
    "    root = root.resolve()\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "    if run_name:\n",
    "        base = run_name.strip().replace(\" \", \"_\") or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    else:\n",
    "        base = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = root / base\n",
    "    suffix = 1\n",
    "    while run_dir.exists():\n",
    "        run_dir = root / f\"{base}_{suffix:02d}\"\n",
    "        suffix += 1\n",
    "    run_dir.mkdir()\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "def serialize_config(\n",
    "    args: argparse.Namespace,\n",
    "    train_count: int,\n",
    "    val_count: int,\n",
    "    test_count: int,\n",
    ") -> dict:\n",
    "    return {\n",
    "        \"metadata\": str(args.metadata),\n",
    "        \"root\": str(args.root),\n",
    "        \"epochs\": args.epochs,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"max_depth\": args.max_depth,\n",
    "        \"subsample\": args.subsample,\n",
    "        \"colsample_bytree\": args.colsample_bytree,\n",
    "        \"reg_alpha\": args.reg_alpha,\n",
    "        \"reg_lambda\": args.reg_lambda,\n",
    "        \"min_child_weight\": args.min_child_weight,\n",
    "        \"tree_method\": args.tree_method,\n",
    "        \"val_split\": args.val_split,\n",
    "        \"test_split\": args.test_split,\n",
    "        \"seed\": args.seed,\n",
    "        \"train_samples\": train_count,\n",
    "        \"val_samples\": val_count,\n",
    "        \"test_samples\": test_count,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23231e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv: Optional[Sequence[str]] = None) -> None:\n",
    "    args = parse_args(argv)\n",
    "\n",
    "    if args.epochs <= 0:\n",
    "        raise SystemExit(\"epochs must be positive.\")\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    npz_paths = load_metadata(args.metadata, args.root)\n",
    "    X, y = load_dataset(npz_paths)\n",
    "\n",
    "    if args.test_split > 0.0:\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=args.test_split,\n",
    "            random_state=args.seed,\n",
    "        )\n",
    "    else:\n",
    "        X_temp, y_temp = X, y\n",
    "        X_test, y_test = None, None\n",
    "\n",
    "    if args.val_split > 0.0:\n",
    "        denom = 1.0 - args.test_split\n",
    "        denom = denom if denom > 0 else 1.0\n",
    "        val_fraction = args.val_split / denom\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp,\n",
    "            y_temp,\n",
    "            test_size=val_fraction,\n",
    "            random_state=args.seed,\n",
    "        )\n",
    "    else:\n",
    "        X_train, y_train = X_temp, y_temp\n",
    "        X_val, y_val = None, None\n",
    "\n",
    "    eval_set: List[Tuple[np.ndarray, np.ndarray]] = [(X_train, y_train)]\n",
    "    if X_val is not None:\n",
    "        eval_set.append((X_val, y_val))\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_depth=args.max_depth,\n",
    "        subsample=args.subsample,\n",
    "        colsample_bytree=args.colsample_bytree,\n",
    "        reg_alpha=args.reg_alpha,\n",
    "        reg_lambda=args.reg_lambda,\n",
    "        min_child_weight=args.min_child_weight,\n",
    "        random_state=args.seed,\n",
    "        tree_method=args.tree_method,\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=[\"rmse\", \"mae\"],\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=eval_set,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    evals_result = model.evals_result()\n",
    "    train_rmse_history = [float(v) for v in evals_result[\"validation_0\"][\"rmse\"]]\n",
    "    num_epochs = len(train_rmse_history)\n",
    "    if num_epochs == 0:\n",
    "        raise SystemExit(\"No training history recorded by XGBoost.\")\n",
    "\n",
    "    if X_val is not None:\n",
    "        val_rmse_history = [float(v) for v in evals_result[\"validation_1\"][\"rmse\"]]\n",
    "        val_mae_history = [float(v) for v in evals_result[\"validation_1\"][\"mae\"]]\n",
    "    else:\n",
    "        val_rmse_history = [float(\"nan\")] * num_epochs\n",
    "        val_mae_history = [float(\"nan\")] * num_epochs\n",
    "\n",
    "    run_dir = create_run_dir(args.experiment_root, args.run_name)\n",
    "    history: List[dict] = []\n",
    "\n",
    "    config = serialize_config(\n",
    "        args,\n",
    "        train_count=len(X_train),\n",
    "        val_count=len(X_val) if X_val is not None else 0,\n",
    "        test_count=len(X_test) if X_test is not None else 0,\n",
    "    )\n",
    "    with (run_dir / \"config.json\").open(\"w\", encoding=\"utf-8\") as cfg_file:\n",
    "        json.dump(config, cfg_file, indent=2)\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        epoch = epoch_idx + 1\n",
    "        progress = (epoch_idx + 1) / num_epochs\n",
    "        train_mse = float(train_rmse_history[epoch_idx]) ** 2\n",
    "        val_rmse = float(val_rmse_history[epoch_idx])\n",
    "        val_mae = float(val_mae_history[epoch_idx])\n",
    "        val_mse = val_rmse ** 2 if not np.isnan(val_rmse) else float(\"nan\")\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"{progress * 100:6.2f}% of epoch complete \"\n",
    "            f\"(step {epoch}/{num_epochs})\"\n",
    "        )\n",
    "        if np.isnan(val_mse) or np.isnan(val_mae):\n",
    "            msg = f\"Epoch {epoch:03d} - train MSE: {train_mse:.4f}, val MSE: nan, val MAE: nan\"\n",
    "        else:\n",
    "            msg = (\n",
    "                f\"Epoch {epoch:03d} - train MSE: {train_mse:.4f}, \"\n",
    "                f\"val MSE: {val_mse:.4f}, val MAE: {val_mae:.4f}\"\n",
    "            )\n",
    "        print(f\"{progress * 100:6.2f}% | {msg}\")\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_mse\": float(train_mse),\n",
    "                \"val_mse\": float(val_mse),\n",
    "                \"val_mae\": float(val_mae),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metrics_path = run_dir / \"metrics.csv\"\n",
    "    with metrics_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as metrics_file:\n",
    "        writer = csv.DictWriter(\n",
    "            metrics_file,\n",
    "            fieldnames=[\"epoch\", \"train_mse\", \"val_mse\", \"val_mae\"],\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(history)\n",
    "\n",
    "    scatter_path: Optional[Path] = None\n",
    "    test_metrics_path: Optional[Path] = None\n",
    "    if X_test is not None and X_test.size > 0:\n",
    "        test_preds = model.predict(X_test)\n",
    "        test_mse = float(mean_squared_error(y_test, test_preds))\n",
    "        test_mae = float(mean_absolute_error(y_test, test_preds))\n",
    "        print(f\"Test MSE: {test_mse:.4f} | Test MAE: {test_mae:.4f}\")\n",
    "        test_metrics_path = run_dir / \"test_metrics.json\"\n",
    "        with test_metrics_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump({\"mse\": test_mse, \"mae\": test_mae}, fh, indent=2)\n",
    "\n",
    "        scatter_path = run_dir / \"test_scatter.png\"\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.scatter(y_test, test_preds, s=12, alpha=0.6, edgecolors=\"none\")\n",
    "        min_val = float(min(test_preds.min(), y_test.min()))\n",
    "        max_val = float(max(test_preds.max(), y_test.max()))\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], color=\"gray\", linestyle=\"--\")\n",
    "        ax.set_xlabel(\"True Count\")\n",
    "        ax.set_ylabel(\"Predicted Count\")\n",
    "        ax.set_title(\"Test Predictions vs. Ground Truth\")\n",
    "        ax.grid(True, alpha=0.2)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(scatter_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "    model_path = run_dir / \"model.pt\"\n",
    "    model.save_model(str(model_path))\n",
    "    print(f\"Saved model weights to {model_path}\")\n",
    "    print(f\"Metrics logged to {metrics_path}\")\n",
    "    if test_metrics_path is not None:\n",
    "        print(f\"Test metrics saved to {test_metrics_path}\")\n",
    "    if scatter_path is not None:\n",
    "        print(f\"Test scatter plot saved to {scatter_path}\")\n",
    "    print(f\"Run artifacts stored in {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training with lighter defaults to reduce memory load.\n",
    "main([\"--epochs\", \"80\", \"--max-depth\", \"4\", \"--subsample\", \"0.5\", \"--colsample-bytree\", \"0.5\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

from __future__ import annotations

import argparse
import csv
import json
import random
from datetime import datetime
from pathlib import Path
from typing import List, Sequence

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train a Random Forest regressor for cell counting."
    )
    parser.add_argument(
        "--metadata",
        type=Path,
        default=Path("processed/metadata.csv"),
        help="CSV generated by data_preprocess.py.",
    )
    parser.add_argument(
        "--root",
        type=Path,
        default=Path("processed"),
        help="Root directory containing sample NPZ files.",
    )
    parser.add_argument(
        "--experiment-root",
        type=Path,
        default=Path("experiments"),
        help="Directory where run artifacts will be stored.",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Optional identifier for this training run (defaults to timestamp).",
    )
    parser.add_argument(
        "--val-split",
        type=float,
        default=0.1,
        help="Validation split fraction.",
    )
    parser.add_argument(
        "--test-split",
        type=float,
        default=0.1,
        help="Test split fraction.",
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Number of trees in the forest.",
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=16,
        help="Maximum tree depth (None implies unlimited).",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed.",
    )
    return parser.parse_args()


def load_metadata(metadata_path: Path, root: Path) -> List[Path]:
    npz_paths: List[Path] = []
    with metadata_path.open("r", newline="", encoding="utf-8") as fh:
        reader = csv.DictReader(fh)
        for row in reader:
            sample_rel = Path(row["sample_npz"])
            if sample_rel.is_absolute():
                sample_path = sample_rel
            elif sample_rel.parts and sample_rel.parts[0] == root.name:
                sample_path = (root.parent / sample_rel).resolve()
            else:
                sample_path = (root / sample_rel).resolve()
            npz_paths.append(sample_path)
    if not npz_paths:
        raise SystemExit(f"No samples found via {metadata_path}.")
    return npz_paths


def load_dataset(npz_paths: Sequence[Path]) -> tuple[np.ndarray, np.ndarray]:
    features: List[np.ndarray] = []
    targets: List[float] = []
    for path in npz_paths:
        with np.load(path) as data:
            image = data["image"]
            features.append(image.reshape(-1))
            targets.append(float(np.array(data["count"]).item()))
    X = np.stack(features).astype(np.float32)
    y = np.asarray(targets, dtype=np.float32)
    return X, y


def create_run_dir(root: Path, run_name: str | None) -> Path:
    root = root.resolve()
    root.mkdir(parents=True, exist_ok=True)
    if run_name:
        base = run_name.strip().replace(" ", "_") or datetime.now().strftime(
            "%Y%m%d_%H%M%S"
        )
    else:
        base = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = root / base
    suffix = 1
    while run_dir.exists():
        run_dir = root / f"{base}_{suffix:02d}"
        suffix += 1
    run_dir.mkdir()
    return run_dir


def main() -> None:
    args = parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)

    npz_paths = load_metadata(args.metadata, args.root)
    X, y = load_dataset(npz_paths)

    X_temp, X_test, y_temp, y_test = train_test_split(
        X,
        y,
        test_size=args.test_split,
        random_state=args.seed,
    )
    val_fraction = args.val_split / (1.0 - args.test_split) if args.test_split < 1 else 0
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp,
        y_temp,
        test_size=val_fraction,
        random_state=args.seed,
    )

    rf = RandomForestRegressor(
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        random_state=args.seed,
        verbose=True,
        n_jobs=-1,
    )
    rf.fit(X_train, y_train)

    def evaluate(split_name: str, X_split: np.ndarray, y_split: np.ndarray) -> dict:
        preds = rf.predict(X_split)
        mse = float(mean_squared_error(y_split, preds))
        mae = float(mean_absolute_error(y_split, preds))
        return {"split": split_name, "mse": mse, "mae": mae}

    metrics = [
        evaluate("train", X_train, y_train),
        evaluate("val", X_val, y_val),
        evaluate("test", X_test, y_test),
    ]

    run_dir = create_run_dir(args.experiment_root, args.run_name)
    config = {
        "metadata": str(args.metadata),
        "root": str(args.root),
        "n_estimators": args.n_estimators,
        "max_depth": args.max_depth,
        "val_split": args.val_split,
        "test_split": args.test_split,
        "seed": args.seed,
    }
    with (run_dir / "config.json").open("w", encoding="utf-8") as fh:
        json.dump(config, fh, indent=2)

    metrics_csv = run_dir / "metrics.csv"
    with metrics_csv.open("w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(fh, fieldnames=["split", "mse", "mae"])
        writer.writeheader()
        writer.writerows(metrics)

    model_path = run_dir / "model.joblib"
    try:
        import joblib  # type: ignore
    except ImportError:  # pragma: no cover
        raise SystemExit("joblib is required; install via `pip install joblib`.") from None
    joblib.dump(rf, model_path)

    print(f"Saved model to {model_path}")
    print(f"Metrics written to {metrics_csv}")
    print(f"Run artifacts stored in {run_dir}")


if __name__ == "__main__":
    main()

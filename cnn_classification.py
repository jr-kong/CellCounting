from __future__ import annotations

import argparse
import csv
import json
import random
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Sequence, Tuple

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.metrics import (
    ConfusionMatrixDisplay,
    auc,
    f1_score,
    precision_score,
    roc_auc_score,
    roc_curve,
)
from torch import nn
from torch.utils.data import DataLoader, Dataset, Subset


@dataclass(frozen=True)
class SampleEntry:
    npz_path: Path
    count: int


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Train a CNN regressor and report tolerance-based classification metrics."
        )
    )
    parser.add_argument(
        "--metadata",
        type=Path,
        default=Path("/content/drive/MyDrive/CellCounting/processed/metadata.csv"),
        help="CSV generated by data_preprocess.py.",
    )
    parser.add_argument(
        "--root",
        type=Path,
        default=Path("/content/drive/MyDrive/CellCounting/processed"),
        help="Root directory containing sample NPZ files.",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="Mini-batch size.",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=10,
        help="Number of training epochs.",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-3,
        help="Adam learning rate.",
    )
    parser.add_argument(
        "--val-split",
        type=float,
        default=0.1,
        help="Fraction of samples reserved for validation (0 disables split).",
    )
    parser.add_argument(
        "--test-split",
        type=float,
        default=0.1,
        help="Fraction of samples reserved for final test evaluation.",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=0,
        help="Dataloader worker processes.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for shuffling and weight init.",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Training device (cuda or cpu).",
    )
    parser.add_argument(
        "--experiment-root",
        type=Path,
        default=Path("/content/drive/MyDrive/CellCounting/experiments_cnn_cls"),
        help="Directory where run artifacts will be stored.",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Optional identifier for this training run (defaults to timestamp).",
    )
    parser.add_argument(
        "--tolerance",
        type=float,
        default=2.0,
        help="Tolerance window around the ground-truth count to mark predictions correct.",
    )
    return parser.parse_args()


def load_metadata(metadata_path: Path, root: Path) -> List[SampleEntry]:
    entries: List[SampleEntry] = []
    with metadata_path.open("r", newline="", encoding="utf-8") as fh:
        reader = csv.DictReader(fh)
        for row in reader:
            sample_rel = Path(row["sample_npz"])
            count = int(row["cell_count"])
            if sample_rel.is_absolute():
                sample_path = sample_rel
            elif sample_rel.parts and sample_rel.parts[0] == root.name:
                sample_path = (root.parent / sample_rel).resolve()
            else:
                sample_path = (root / sample_rel).resolve()
            entries.append(SampleEntry(sample_path, count))
    if not entries:
        raise SystemExit(f"No samples found via {metadata_path}.")
    return entries


class DatasetClass(Dataset):
    def __init__(self, entries: Sequence[SampleEntry]):
        self.entries = list(entries)

    def __len__(self) -> int:
        return len(self.entries)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        entry = self.entries[idx]
        with np.load(entry.npz_path) as data:
            image = data["image"].astype(np.float32)
            count = float(np.array(data["count"]).item())
        image_tensor = torch.from_numpy(image)
        target = torch.tensor(count, dtype=torch.float32)
        return image_tensor, target


class CNN(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
        )
        self.regressor = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        feats = self.features(x)
        return self.regressor(feats).squeeze(1)


def split_indices(
    n_samples: int, val_split: float, test_split: float, seed: int
) -> Tuple[List[int], List[int], List[int]]:
    if val_split < 0 or test_split < 0:
        raise SystemExit("Splits must be non-negative.")
    if val_split + test_split >= 1.0:
        raise SystemExit("val_split + test_split must be less than 1.")

    indices = list(range(n_samples))
    random.Random(seed).shuffle(indices)

    test_count = int(round(n_samples * test_split))
    val_count = int(round(n_samples * val_split))

    test_indices = indices[:test_count]
    val_indices = indices[test_count : test_count + val_count]
    train_indices = indices[test_count + val_count :]

    if not train_indices:
        raise SystemExit("Splits leave no training samples.")

    return train_indices, val_indices, test_indices


def create_run_dir(root: Path, run_name: Optional[str]) -> Path:
    root = root.resolve()
    root.mkdir(parents=True, exist_ok=True)
    if run_name:
        base = run_name.strip().replace(" ", "_") or datetime.now().strftime(
            "%Y%m%d_%H%M%S"
        )
    else:
        base = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = root / base
    suffix = 1
    while run_dir.exists():
        run_dir = root / f"{base}_{suffix:02d}"
        suffix += 1
    run_dir.mkdir()
    return run_dir


def serialize_config(
    args: argparse.Namespace,
    device: torch.device,
    train_count: int,
    val_count: int,
    test_count: int,
) -> dict:
    config = {}
    for key, value in vars(args).items():
        if isinstance(value, Path):
            config[key] = str(value)
        else:
            config[key] = value
    config["resolved_device"] = device.type
    config["train_samples"] = int(train_count)
    config["val_samples"] = int(val_count)
    config["test_samples"] = int(test_count)
    return config


def collect_predictions(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
) -> Tuple[np.ndarray, np.ndarray]:
    model.eval()
    preds_batches: List[np.ndarray] = []
    targets_batches: List[np.ndarray] = []
    with torch.no_grad():
        for images, targets in loader:
            images = images.to(device)
            targets = targets.to(device)
            preds = model(images)
            preds_batches.append(preds.cpu().numpy())
            targets_batches.append(targets.cpu().numpy())
    if not preds_batches:
        return np.empty((0,), dtype=np.float32), np.empty((0,), dtype=np.float32)
    return np.concatenate(targets_batches), np.concatenate(preds_batches)


def compute_tolerance_metrics(
    y_true: np.ndarray, y_pred: np.ndarray, tolerance: float
) -> dict:
    if y_true.size == 0:
        return {
            "sensitivity": float("nan"),
            "specificity": float("nan"),
            "f1": float("nan"),
            "roc_auc": float("nan"),
            "precision": float("nan"),
            "accuracy": float("nan"),
            "within_tolerance": float("nan"),
            "confusion_matrix": np.zeros((2, 2), dtype=int),
            "decision": np.empty((0,), dtype=int),
            "prob": np.empty((0,), dtype=float),
        }

    diff = np.abs(y_pred - y_true)
    y_true_binary = (diff <= tolerance).astype(int)

    scale = max(tolerance / np.log(2), 1e-6)
    prob_scores = np.exp(-diff / scale)
    prob_scores = np.clip(prob_scores, 1e-7, 1.0 - 1e-7)
    y_pred_binary = (prob_scores >= 0.5).astype(int)

    tp = int(np.sum((y_pred_binary == 1) & (y_true_binary == 1)))
    tn = int(np.sum((y_pred_binary == 0) & (y_true_binary == 0)))
    fp = int(np.sum((y_pred_binary == 1) & (y_true_binary == 0)))
    fn = int(np.sum((y_pred_binary == 0) & (y_true_binary == 1)))
    conf_mat = np.array([[tn, fp], [fn, tp]], dtype=int)

    sensitivity = float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0
    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0
    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
    accuracy = float((y_pred_binary == y_true_binary).mean())
    within_tolerance = float(y_true_binary.mean())

    try:
        roc_auc = roc_auc_score(y_true_binary, prob_scores)
    except ValueError:
        roc_auc = float("nan")

    return {
        "sensitivity": sensitivity,
        "specificity": specificity,
        "precision": float(precision),
        "f1": float(f1),
        "roc_auc": float(roc_auc),
        "accuracy": accuracy,
        "within_tolerance": within_tolerance,
        "confusion_matrix": conf_mat,
        "decision": y_pred_binary,
        "prob": prob_scores,
    }


def plot_confusion_matrix(cm: np.ndarray, path: Path) -> None:
    fig, ax = plt.subplots(figsize=(5, 4))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Outside", "Within"])
    disp.plot(ax=ax, cmap="Blues", colorbar=False)
    ax.set_title("Tolerance Confusion Matrix")
    fig.tight_layout()
    fig.savefig(path, dpi=200)
    plt.close(fig)


def plot_tolerance_roc_curve(
    y_true_binary: np.ndarray, prob_scores: np.ndarray, path: Path
) -> bool:
    if len(np.unique(y_true_binary)) < 2:
        return False
    try:
        fpr, tpr, _ = roc_curve(y_true_binary, prob_scores)
    except ValueError:
        return False
    curve_auc = auc(fpr, tpr)
    fig, ax = plt.subplots(figsize=(5, 4))
    ax.plot(fpr, tpr, label=f"ROC curve (AUC = {curve_auc:.3f})")
    ax.plot([0, 1], [0, 1], linestyle="--", color="gray", label="Chance")
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("Tolerance ROC Curve")
    ax.grid(True, alpha=0.2)
    ax.legend(loc="lower right")
    fig.tight_layout()
    fig.savefig(path, dpi=200)
    plt.close(fig)
    return True


def main() -> None:
    args = parse_args()

    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)

    entries = load_metadata(args.metadata, args.root)
    dataset = DatasetClass(entries)

    train_idx, val_idx, test_idx = split_indices(
        len(dataset), args.val_split, args.test_split, args.seed
    )
    train_subset = Subset(dataset, train_idx)
    val_subset = Subset(dataset, val_idx) if val_idx else None
    test_subset = Subset(dataset, test_idx) if test_idx else None

    device = torch.device(args.device)
    use_cuda = device.type == "cuda"

    train_loader = DataLoader(
        train_subset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=use_cuda,
    )
    val_loader = (
        DataLoader(
            val_subset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.num_workers,
            pin_memory=use_cuda,
        )
        if val_subset
        else None
    )
    test_loader = (
        DataLoader(
            test_subset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.num_workers,
            pin_memory=use_cuda,
        )
        if test_subset
        else None
    )

    model = CNN().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.MSELoss()

    run_dir = create_run_dir(args.experiment_root, args.run_name)
    history: List[dict] = []

    config = serialize_config(
        args,
        device,
        train_count=len(train_subset),
        val_count=len(val_subset) if val_subset else 0,
        test_count=len(test_subset) if test_subset else 0,
    )
    with (run_dir / "config.json").open("w", encoding="utf-8") as cfg_file:
        json.dump(config, cfg_file, indent=2)

    for epoch in range(1, args.epochs + 1):
        model.train()
        epoch_loss = 0.0
        total_steps = len(train_loader)
        next_print_fraction = 0.1

        for step_idx, (images, targets) in enumerate(train_loader, 1):
            images = images.to(device)
            targets = targets.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = criterion(preds, targets)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item() * targets.size(0)

            progress = step_idx / total_steps if total_steps else 1.0
            if progress >= next_print_fraction or step_idx == total_steps:
                print(
                    f"Epoch {epoch:03d} | "
                    f"{progress * 100:6.2f}% of epoch complete "
                    f"(step {step_idx}/{total_steps})"
                )
                next_print_fraction += 0.1

        train_mse = epoch_loss / len(train_subset)
        msg = f"Epoch {epoch:03d} - train MSE: {train_mse:.4f}"

        if val_loader is not None:
            val_targets, val_preds = collect_predictions(model, val_loader, device)
            if val_targets.size > 0:
                diff = val_preds - val_targets
                val_mse = float(np.mean(diff ** 2))
                val_mae = float(np.mean(np.abs(diff)))
                tolerance_metrics = compute_tolerance_metrics(
                    val_targets, val_preds, args.tolerance
                )
                msg += (
                    f", val MSE: {val_mse:.4f}, val MAE: {val_mae:.4f}, "
                    f"val sens: {tolerance_metrics['sensitivity']:.4f}, "
                    f"val spec: {tolerance_metrics['specificity']:.4f}, "
                    f"val f1: {tolerance_metrics['f1']:.4f}, "
                    f"val ROC AUC: {tolerance_metrics['roc_auc']:.4f}"
                )
            else:
                val_mse = val_mae = float("nan")
                tolerance_metrics = {
                    "sensitivity": float("nan"),
                    "specificity": float("nan"),
                    "f1": float("nan"),
                    "roc_auc": float("nan"),
                }
        else:
            val_mse = val_mae = float("nan")
            tolerance_metrics = {
                "sensitivity": float("nan"),
                "specificity": float("nan"),
                "f1": float("nan"),
                "roc_auc": float("nan"),
            }

        percent = (epoch / args.epochs) * 100.0
        print(f"{percent:6.2f}% | {msg}")
        history.append(
            {
                "epoch": epoch,
                "train_mse": float(train_mse),
                "val_mse": float(val_mse),
                "val_mae": float(val_mae),
                "val_sensitivity": float(tolerance_metrics["sensitivity"]),
                "val_specificity": float(tolerance_metrics["specificity"]),
                "val_f1": float(tolerance_metrics["f1"]),
                "val_roc_auc": float(tolerance_metrics["roc_auc"]),
            }
        )

    metrics_path = run_dir / "metrics.csv"
    with metrics_path.open("w", newline="", encoding="utf-8") as metrics_file:
        writer = csv.DictWriter(
            metrics_file,
            fieldnames=[
                "epoch",
                "train_mse",
                "val_mse",
                "val_mae",
                "val_sensitivity",
                "val_specificity",
                "val_f1",
                "val_roc_auc",
            ],
        )
        writer.writeheader()
        writer.writerows(history)

    test_metrics_path = None
    confusion_path = None
    roc_curve_path = None
    test_targets = np.empty((0,), dtype=np.float32)
    test_preds = np.empty((0,), dtype=np.float32)
    if test_loader is not None:
        test_targets, test_preds = collect_predictions(model, test_loader, device)
        if test_targets.size > 0:
            diff = test_preds - test_targets
            test_mse = float(np.mean(diff ** 2))
            test_mae = float(np.mean(np.abs(diff)))
            tolerance_metrics = compute_tolerance_metrics(
                test_targets, test_preds, args.tolerance
            )
            print(
                f"Test MSE: {test_mse:.4f} | Test MAE: {test_mae:.4f} | "
                f"Test sens: {tolerance_metrics['sensitivity']:.4f} | "
                f"Test spec: {tolerance_metrics['specificity']:.4f} | "
                f"Test f1: {tolerance_metrics['f1']:.4f} | "
                f"Test ROC AUC: {tolerance_metrics['roc_auc']:.4f}"
            )
            test_metrics_path = run_dir / "test_metrics.json"
            with test_metrics_path.open("w", encoding="utf-8") as fh:
                json.dump(
                    {
                        "mse": test_mse,
                        "mae": test_mae,
                        "sensitivity": tolerance_metrics["sensitivity"],
                        "specificity": tolerance_metrics["specificity"],
                        "precision": tolerance_metrics["precision"],
                        "f1": tolerance_metrics["f1"],
                        "roc_auc": tolerance_metrics["roc_auc"],
                        "accuracy": tolerance_metrics["accuracy"],
                        "within_tolerance": tolerance_metrics["within_tolerance"],
                    },
                    fh,
                    indent=2,
                )

            confusion_path = run_dir / "tolerance_confusion_matrix.png"
            plot_confusion_matrix(tolerance_metrics["confusion_matrix"], confusion_path)

            roc_curve_path = run_dir / "tolerance_roc_curve.png"
            if not plot_tolerance_roc_curve(
                (np.abs(test_preds - test_targets) <= args.tolerance).astype(int),
                tolerance_metrics["prob"],
                roc_curve_path,
            ):
                roc_curve_path = None
        else:
            test_mse = test_mae = float("nan")

    model_path = run_dir / "model.pt"
    torch.save(model.state_dict(), model_path)
    print(f"Saved model weights to {model_path}")
    print(f"Metrics logged to {metrics_path}")
    if test_metrics_path is not None:
        print(f"Test metrics saved to {test_metrics_path}")
    if confusion_path is not None:
        print(f"Confusion matrix saved to {confusion_path}")
    if roc_curve_path is not None:
        print(f"ROC curve saved to {roc_curve_path}")
    if test_targets.size > 0:
        scatter_path = run_dir / "test_regression_scatter.png"
        fig, ax = plt.subplots(figsize=(6, 6))
        ax.scatter(test_targets, test_preds, s=12, alpha=0.6, edgecolors="none")
        min_val = float(min(test_targets.min(), test_preds.min()))
        max_val = float(max(test_targets.max(), test_preds.max()))
        ax.plot([min_val, max_val], [min_val, max_val], color="gray", linestyle="--")
        ax.set_xlabel("True Count")
        ax.set_ylabel("Predicted Count")
        ax.set_title("Test Predictions vs. Ground Truth")
        ax.grid(True, alpha=0.2)
        fig.tight_layout()
        fig.savefig(scatter_path, dpi=200)
        plt.close(fig)
        print(f"Regression scatter saved to {scatter_path}")
    print(f"Run artifacts stored in {run_dir}")


if __name__ == "__main__":
    main()
